{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a38d49-423a-4394-b003-b3faa1ec90e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import umap\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f19cd1f9-b43b-4199-ac90-c92c2396a755",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runnig through the \"\"\"\"example\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412baa73-93d7-448f-b9d5-4251f087614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "TODO:\n",
    "# Author: \n",
    "# Created Time : \n",
    "\n",
    "# File Name: \n",
    "# Description: \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='TODO')\n",
    "\n",
    "    # IO and norm options\n",
    "    parser.add_argument('--exp', '-e', type=str, help='TODO: Input gene expression data path')\n",
    "    parser.add_argument('--adj', '-a', type=str, help='Input adjacency matrix data path')\n",
    "    parser.add_argument('--coordinate', '-c', type=str, help='Input cell coordinate data path')\n",
    "    parser.add_argument('--reference', '-r', type=str, help='Input cell type label path')\n",
    "    parser.add_argument('--verbose', action='store_true', help='Print loss of training process')\n",
    "    parser.add_argument('--outdir', '-o', type=str, default='output/', help='Output path')\n",
    "    parser.add_argument('--outpostfix', '-n', type=str, help='The postfix of the output file')\n",
    "    parser.add_argument('--log_add_number', type=int, default=None, help='Perform log10(x+log_add_number) transform')\n",
    "    parser.add_argument('--fil_gene', type=int, default=None, help='Remove genes expressed in less than fil_gene cells')\n",
    "    parser.add_argument('--latent_feature', '-l', default=None, help='')\n",
    "\n",
    "    # Training options\n",
    "    parser.add_argument('--test_ratio', '-t', type=int, default=0.1, help='Testing set ratio (>1: edge number, <1: edge ratio; default: 0.1)')    \n",
    "    parser.add_argument('--iteration', '-i', type=int, default=2, help='Iteration (default: 40)')\n",
    "    parser.add_argument('--encode_dim', type=int, nargs=2, default=[125, 125], help='Encoder structure')\n",
    "    parser.add_argument('--regularization_dim', type=int, nargs=2, default=[150, 125], help='Adversarial regularization structure') #TODO:[125, 125, ]\n",
    "    parser.add_argument('--lr1', type=float, default=0.0004, help='TODO')\n",
    "    parser.add_argument('--lr2', type=float, default=0.0008, help='TODO')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0, help='Weight for L2 loss on latent features')\n",
    "    parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate (1 - keep probability)')\n",
    "    parser.add_argument('--features', type=int, default=1, help='Whether to use features (1) or not (0)')\n",
    "    parser.add_argument('--seed', type=int, default=7, help='Random seed for repeat results') #TODO:50,7,1\n",
    "    parser.add_argument('--activation', type=str, default='relu', help=\"Activation function of hidden units (default: relu)\")\n",
    "    parser.add_argument('--init', type=str, default='glorot_uniform', help=\"Initialization method for weights (default: glorot_uniform)\")\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help=\"Optimization method (default: Adam)\")\n",
    "\n",
    "    # Clustering options\n",
    "    parser.add_argument('--cluster', action='store_true', help='TODO')\n",
    "    parser.add_argument('--cluster_num', type=int, default=None, help='TODO')    \n",
    "\n",
    "    parser.add_argument('--gpu', '-g', type=int, default=0, help='Select gpu device number for training')\n",
    "    # tf.test.is_built_with_cuda()\n",
    "\n",
    "    # parser.set_defaults(transpose=False,\n",
    "    #                     testsplit=False,\n",
    "    #                     saveweights=False,\n",
    "    #                     sizefactors=True,\n",
    "    #                     batchnorm=True,\n",
    "    #                     checkcounts=True,\n",
    "    #                     norminput=True,\n",
    "    #                     hyper=False,\n",
    "    #                     debug=False,\n",
    "    #                     tensorboard=False,\n",
    "    #                     loginput=True)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "\n",
    "    # Select gpu device number  \n",
    "    import os \n",
    "    try:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)  #if you use GPU, you must be sure that there is at least one GPU available in your device\n",
    "    except:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  #set only using cpu\n",
    "\n",
    "    # Import modules\n",
    "    try:\n",
    "        import tensorflow as tf  #import tf and the rest module after parse_args() to make argparse help show faster\n",
    "    except ImportError:\n",
    "        raise ImportError('DeepLinc requires TensorFlow. Please follow instructions'\n",
    "                          ' at https://www.tensorflow.org/install/ to install'\n",
    "                          ' it.')\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy.sparse as sp\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    import copy\n",
    "    from deeplinc.io import *\n",
    "    from deeplinc.plot import *\n",
    "    from deeplinc.utils import sparse2tuple, packed_data, set_placeholder, set_optimizer, update, ranked_partial\n",
    "    from deeplinc.models import Deeplinc, Discriminator\n",
    "    from deeplinc.metrics import linkpred_metrics, select_optimal_threshold\n",
    "    from deeplinc.enrichment import connection_number_between_groups, generate_adj_new_long_edges, edges_enrichment_evaluation\n",
    "    from deeplinc.sensitivity import get_sensitivity\n",
    "    from deeplinc.cluster import clustering\n",
    "\n",
    "    # Set random seed\n",
    "    seed = args.seed\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    # Import and pack datasets\n",
    "    exp_df, adj_df = read_dataset(args.exp, args.adj, args.fil_gene, args.log_add_number)\n",
    "    exp, adj = exp_df.values, adj_df.values\n",
    "    coord_df = read_coordinate(args.coordinate)\n",
    "    coord = coord_df.values\n",
    "    cell_label_df = read_cell_label(args.reference)\n",
    "    cell_label = cell_label_df.values\n",
    "\n",
    "    feas = packed_data(exp, adj, args.test_ratio)\n",
    "    var_placeholders = set_placeholder(feas['adj_train'], args.encode_dim[1])\n",
    "\n",
    "    # Output some basic information\n",
    "    cell_num = exp.shape[0]\n",
    "    gene_num = exp.shape[1]\n",
    "    predefined_edge_num = np.where(adj==1)[0].shape[0]/2\n",
    "\n",
    "    print(\"\\n**************************************************************************************************************\")\n",
    "    print(\"  DeepLinc: De novo reconstruction of cell interaction landscapes from single-cell spatial transcriptome data  \")\n",
    "    print(\"**************************************************************************************************************s\\n\")\n",
    "    print(\"======== Parameters ========\")\n",
    "    print('Cell number: {}\\nGene number: {}\\nPredefined local connection number: {}\\niteration: {}'.format(\n",
    "            cell_num, gene_num, predefined_edge_num, args.iteration))\n",
    "    print(\"============================\")\n",
    "\n",
    "    # Create storage folders\n",
    "    os.mkdir(\"./model\")\n",
    "\n",
    "    # Building model and optimizer\n",
    "    # dims = []\n",
    "    deeplinc = Deeplinc(var_placeholders, feas['num_features'], feas['num_nodes'], feas['features_nonzero'], args.encode_dim[0], args.encode_dim[1])\n",
    "    deeplinc_discriminator = Discriminator(args.encode_dim[1], args.regularization_dim[0], args.regularization_dim[1])\n",
    "    opt = set_optimizer(deeplinc, deeplinc_discriminator, var_placeholders, feas['pos_weight'], feas['norm'], feas['num_nodes'], args.lr1, args.lr2)\n",
    "\n",
    "################################################################################################################\n",
    "    # Fitting model\n",
    "    # Saver\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    # Initialize session\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Metrics list\n",
    "    train_loss = []\n",
    "    test_ap = []\n",
    "    # latent_feature = None\n",
    "    max_test_ap_score = 0\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(args.iteration):\n",
    "\n",
    "        emb_hidden1_train, emb_hidden2_train, avg_cost_train = update(deeplinc, opt, sess, feas['adj_norm'], feas['adj_label'], feas['features'], var_placeholders, feas['adj_train'], args.dropout, args.encode_dim[1])\n",
    "        train_loss.append(avg_cost_train)\n",
    "\n",
    "        lm_train = linkpred_metrics(feas['test_edges'], feas['test_edges_false'])\n",
    "\n",
    "        roc_score, ap_score, acc_score, _ = lm_train.get_roc_score(emb_hidden2_train, feas)\n",
    "        test_ap.append(ap_score)\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(avg_cost_train), \"test_roc=\", \"{:.5f}\".format(roc_score), \"test_ap=\", \"{:.5f}\".format(ap_score))\n",
    "\n",
    "        if ap_score > max_test_ap_score:\n",
    "            max_test_ap_score = ap_score\n",
    "\n",
    "            saver.save(sess, './model/'+args.outpostfix, global_step=epoch+1)\n",
    "\n",
    "            np.save(\"emb_hidden1_\"+str(epoch+1)+'.npy', emb_hidden1_train)\n",
    "            np.save(\"emb_hidden2_\"+str(epoch+1)+'.npy', emb_hidden2_train)\n",
    "\n",
    "            latent_feature = copy.deepcopy(emb_hidden2_train)\n",
    "\n",
    "    plot_evaluating_metrics(test_ap, \"epoch\", \"score\", [\"AUPRC\"], \"AUPRC\")\n",
    "    write_pickle(feas, 'feas')\n",
    "\n",
    "################################################################################################################\n",
    "\n",
    "    ### Output ###\n",
    "    # 3.1. 概率、连接和阈值\n",
    "    adj_reconstructed_prob, adj_reconstructed, _, _, all_acc_score, max_acc_score, optimal_threshold = select_optimal_threshold(feas['test_edges'], feas['test_edges_false']).select(latent_feature, feas)\n",
    "    print(optimal_threshold)\n",
    "    print(max_acc_score)\n",
    "\n",
    "    write_json(all_acc_score, 'acc_diff_threshold_'+args.outpostfix)\n",
    "    write_json({'optimal_threshold':optimal_threshold,'max_acc_score':max_acc_score}, 'threshold_'+args.outpostfix)\n",
    "    write_csv_matrix(adj_reconstructed_prob, 'adj_reconstructed_prob_'+args.outpostfix)\n",
    "    write_csv_matrix(adj_reconstructed, 'adj_reconstructed_'+args.outpostfix)\n",
    "\n",
    "    # 3.2. 距离分布、距离矩阵\n",
    "    adj_diff = adj - adj_reconstructed\n",
    "    adj_diff = (adj_diff == -1).astype('int')\n",
    "    adj_diff = sp.csr_matrix(adj_diff)\n",
    "\n",
    "    dist_matrix_rongyu = pdist(coord, 'euclidean')\n",
    "    dist_matrix = squareform(dist_matrix_rongyu)\n",
    "\n",
    "    #这里好像直接adj_diff乘上dist_matrix就可以\n",
    "    new_edges = sparse2tuple(sp.triu(sp.csr_matrix(adj_diff)))[0]\n",
    "    all_new_edges_dist = dist_matrix[new_edges[:,0].tolist(),new_edges[:,1].tolist()]\n",
    "    plot_histogram(all_new_edges_dist, xlabel='distance', ylabel='density', filename='all_new_edges_distance', color=\"coral\")\n",
    "    write_csv_matrix(dist_matrix*adj_diff, 'all_new_edges_dist_matrix')\n",
    "\n",
    "    # 3.3. 连接可视化\n",
    "    id_subgraph, _ = ranked_partial(adj, adj_reconstructed, coord, [10,15])  #返回的是[(diff,[id_list]),(diff,[id_list])...]这种形式\n",
    "                                                                                                #adj_rec1:[10,15], adj_rec2:[3,5]\n",
    "    rank = 0\n",
    "    for item in id_subgraph:\n",
    "        cell_type_subgraph = cell_label[item[1],:][:,[0,1]]\n",
    "        cell_type_subgraph[:,0] = np.array(list(range(cell_type_subgraph.shape[0]))) + 1  #需要对X重新生成细胞的id，这里以1开始\n",
    "        coord_subgraph = coord[item[1],:]\n",
    "        adj_reconstructed_subgraph = adj_reconstructed[item[1],:][:,item[1]]\n",
    "        rank += 1\n",
    "        # adjacency_visualization(cell_type_subgraph, coord_subgraph, adj_reconstructed_subgraph, filename='spatial_network_rank'+str(rank)+'_diff'+str('%.3f'%item[0]))\n",
    "\n",
    "    # 4. 互作强度\n",
    "    cutoff_distance = np.percentile(all_new_edges_dist,99)\n",
    "\n",
    "    connection_number, _ = connection_number_between_groups(adj, cell_label[:,1])\n",
    "    write_csv_matrix(connection_number, 'connection_number_between_groups')\n",
    "\n",
    "    adj_new_long_edges = generate_adj_new_long_edges(dist_matrix, new_edges, all_new_edges_dist, cutoff_distance)\n",
    "    write_csv_matrix(adj_new_long_edges.todense(), 'adj_new_long_edges')\n",
    "\n",
    "    print('------permutations calculating------')\n",
    "    cell_type_name = [np.unique(cell_label[cell_label[:,1]==i,2])[0] for i in np.unique(cell_label[:,1])]\n",
    "    test_result, _, _, _ = edges_enrichment_evaluation(adj, cell_label[:,1], cell_type_name, edge_type='all edges')\n",
    "    write_csv_matrix(test_result, 'all_edges_enrichment_evaluation', colnames=['cell type A','cell type B','average_connectivity','significance'])\n",
    "    test_result, _, _, _ = edges_enrichment_evaluation(adj_new_long_edges.toarray(), cell_label[:,1], cell_type_name, edge_type='long edges', dist_matrix=dist_matrix, cutoff_distance=cutoff_distance)\n",
    "    write_csv_matrix(test_result, 'long_edges_enrichment_evaluation', colnames=['cell type A','cell type B','connection_number','significance'])\n",
    "\n",
    "    # 5. 敏感性\n",
    "    # get_sensitivity(exp_df, feas, './model/'+args.outpostfix)\n",
    "\n",
    "    # 6. 聚类\n",
    "    if args.cluster:\n",
    "        cluster_num = args.cluster_num\n",
    "        cluster_label = clustering(latent_feature, cluster_num)\n",
    "        write_csv_matrix(cluster_label, 'label', colnames=['cell_id','cluster_id'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplinc",
   "language": "python",
   "name": "deeplinc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
